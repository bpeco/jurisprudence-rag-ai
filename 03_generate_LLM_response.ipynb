{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4f9077",
   "metadata": {},
   "source": [
    "# Response Generation\n",
    "\n",
    "This notebook sets up embeddings, loads the index, defines the LLM prompt, and runs the retrieval + response pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60db06",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d882d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3e63a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b800522",
   "metadata": {},
   "source": [
    "## 2. Load embeddings and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce41e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=\"./multivector_chroma_db_001\"\n",
    ")\n",
    "\n",
    "with open(\"parent_documents.pkl\", \"rb\") as f:\n",
    "    parent_documents = pickle.load(f)\n",
    "\n",
    "\n",
    "# üîπ Reconstru√≠s el store\n",
    "store = InMemoryStore()\n",
    "store.mset([(d.metadata[\"id\"], d) for d in parent_documents])\n",
    "\n",
    "# üîπ Reconstru√≠s el retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=\"parent_id\",\n",
    "    search_kwargs={\"k\": 3} # reminder: top 3 similitud para los chunks resumidos, no para los fallos completos\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deafd6e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751f8a7",
   "metadata": {},
   "source": [
    "## 3. Retrieve documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef3dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM\n",
    "llm = ChatOpenAI(model='o4-mini')\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Sos un asistente jur√≠dico.\n",
    "Us√° exclusivamente el contenido de los documentos proporcionados para responder la consulta.\n",
    "Si encontr√°s documentos relacionados, proporcion√° la informaci√≥n correspondiente de dichos documentos.\n",
    "No inventes ni infieras informaci√≥n que no est√© presente en los textos.\n",
    "Si no hay jurisprudencia relevante, indic√° claramente que no la encontr√°s.\n",
    "Tampoco menciones ning√∫n documento si no encontraste juridisprudencia relevante.\n",
    "\n",
    "Documentos:\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\")\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "input_text = \"¬øExiste alg√∫n fallo que trate la inconstitucionalidad de la ley de prenda?\"\n",
    "\n",
    "# Step 1: retrieve relevant documents\n",
    "retrieved_docs = retriever.invoke(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b920e85",
   "metadata": {},
   "source": [
    "## 4. Filter irrelevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262ea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pg/nt5xcr_n6rd6qh7kjj9bnkpr0000gp/T/ipykernel_46343/1521373357.py:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  relevance_chain = LLMChain(llm=llm, prompt=relevance_prompt)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "relevance_prompt = PromptTemplate.from_template(\"\"\"\n",
    "¬øAporta este texto informaci√≥n relevante para responder la pregunta siguiente?\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Texto:\n",
    "{content}\n",
    "\n",
    "Responde √∫nicamente ‚ÄúS√≠‚Äù o ‚ÄúNo‚Äù.\"\"\")\n",
    "# Step 2: filter irrelevant documents\n",
    "relevance_chain = LLMChain(llm=llm, prompt=relevance_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0d8b19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pg/nt5xcr_n6rd6qh7kjj9bnkpr0000gp/T/ipykernel_46343/1178252960.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  verdict = relevance_chain.run(question=input_text, content=doc.page_content)\n"
     ]
    }
   ],
   "source": [
    "filtered_docs = []\n",
    "for doc in retrieved_docs:\n",
    "    verdict = relevance_chain.run(question=input_text, content=doc.page_content)\n",
    "    if verdict.strip().lower().startswith(\"s√≠\"):\n",
    "        filtered_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab038e",
   "metadata": {},
   "source": [
    "## 5. Analyze filtered documents + LLM Final Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64c0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Respuesta del modelo:\n",
      "S√≠. En la causa ‚ÄúHSBC Bank Argentina S.A. c/ Mart√≠nez, Ram√≥n Vicente s/secuestro prendario‚Äù (Fallos 342:1004), la Corte Suprema declar√≥ que el r√©gimen del art. 39 de la Ley 12.962 (prenda con registro) resulta incompatible con los derechos garantizados al consumidor por el art. 42 de la Constituci√≥n Nacional y las normas de la Ley 24.240, y rechaz√≥ su prelaci√≥n sobre el estatuto del consumidor.  \n",
      "  \n",
      "Esa misma doctrina fue luego aplicada por la C√°mara Comercial ‚Äì Sala F en ‚ÄúHSBC Bank Argentina S.A. c/ Garc√≠a, Dora Claudia s/secuestro prendario‚Äù (Expte. COM 5454/2015) y recogida recientemente en ‚ÄúFCA Compa√±√≠a Financiera S.A. c/ Yglesias Rodrigues, Carlos Rubem s/ secuestro prendario‚Äù (Expte. COM 247/2024), donde se declar√≥ inadmisible el secuestro directo sin previa tutela del consumidor, por resultar la norma de la prenda registral ‚Äúincompatible con la vigencia de varias disposiciones que rigen la defensa del derecho de consumidor‚Äù.\n",
      "\n",
      "üìÑ Documentos relevantes encontrados:\n",
      "\n",
      "--- Documento 1 ---\n",
      "Tribunal: CAMARA COMERCIAL - SALA F\n",
      "Sala: SALA F\n",
      "Expediente: COM 000247/2024/CA001\n",
      "Caratula: FCA COMPA√ëIA FINANCIERA S.A. c/ YGLESIAS RODRIGUES, CARLOS RUBEM s/SECUESTRO PRENDARIO\n",
      "Fecha de Sentencia: 01/03/2024\n"
     ]
    }
   ],
   "source": [
    "# Step 3: chain to analyze filtered documents with the question\n",
    "response = combine_docs_chain.invoke({\n",
    "    \"question\": input_text,\n",
    "    \"context\": filtered_docs\n",
    "})\n",
    "\n",
    "print(\"\\nüß† Respuesta del modelo:\")\n",
    "print(response)\n",
    "\n",
    "print(\"\\nüìÑ Documentos relevantes encontrados:\")\n",
    "for i, doc in enumerate(filtered_docs):\n",
    "    print(f\"\\n--- Documento {i+1} ---\")\n",
    "    print(\"Tribunal:\", doc.metadata['Tribunal'])\n",
    "    print(\"Sala:\", doc.metadata['Sala'])\n",
    "    print(\"Expediente:\", doc.metadata['Expediente'])\n",
    "    print(\"Caratula:\", doc.metadata['Caratula'])\n",
    "    print(\"Fecha de Sentencia:\", doc.metadata['FechaSentencia'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c86de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lawyer-agent-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
