# app/pipeline/response.py

from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
import pickle
from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain.storage import InMemoryStore
from langchain.chains.combine_documents import create_stuff_documents_chain



# 1) Defin칤 aqu칤 tu prompt
PROMPT = PromptTemplate.from_template(
    """
Sos un asistente jur칤dico.
Us치 exclusivamente el contenido de los documentos proporcionados para responder la consulta.
Si encontr치s documentos relacionados, proporcion치 la informaci칩n correspondiente de dichos documentos.
No inventes ni infieras informaci칩n que no est칠 presente en los textos.
Si no hay jurisprudencia relevante, indic치 claramente que no la encontr치s.
Tampoco menciones ning칰n documento si no encontraste jurisprudencia relevante.

Documentos:
{context}

Pregunta:
{question}

Respuesta:
"""
)



def get_answer(question: str, k: int = 3) -> tuple[str, list[dict]]:
    """
    Executes the RAG query.
    - question: The text of the question.
    - k: Number of neighboring documents to retrieve.
    Returns a tuple (answer, list_of_metadata).
    """
    # 2) Carg치 el embedding y el vectorstore persistido
    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = Chroma(
        embedding_function=embedding,
        persist_directory="./multivector_chroma_db_001"
    )

    with open("parent_documents.pkl", "rb") as f:
        parent_documents = pickle.load(f)

    # 游댳 Reconstru칤s el store
    store = InMemoryStore()
    store.mset([(d.metadata["id"], d) for d in parent_documents])

    # 游댳 Reconstru칤s el retriever
    retriever = MultiVectorRetriever(
        vectorstore=vectorstore,
        docstore=store,
        id_key="parent_id",
        search_kwargs={"k": k} # reminder: top 3 similitud para los chunks resumidos, no para los fallos completos
    )

    # 4) Arm치 la cadena de RetrievalQA con tu LLM y prompt
    llm = ChatOpenAI(model='o4-mini',temperature=1.0)
    combine_docs_chain = create_stuff_documents_chain(llm=llm, prompt=PROMPT)

    # Step 1: retrieve relevant documents
    retrieved_docs = retriever.invoke(question)

    # Step 2: chain to analyze retrieved documents with the question
    response = combine_docs_chain.invoke({
        "question": question,
        "context": retrieved_docs
    })


    metadatas = [doc.metadata for doc in retrieved_docs]
    return response, metadatas
